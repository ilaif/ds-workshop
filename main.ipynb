{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.avito.ru/files/avito/company/logos/Logo-Avito.png\" alt=\"Avito Logo\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito Demand Prediction Challenge\n",
    "Our challenge is to predict demand for online ads by exposed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Project Documentation](./Project Documentation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### [Original Dataset from Kaggle](https://www.kaggle.com/c/avito-demand-prediction/data)\n",
    "* ### [./helper_data Folder](https://drive.google.com/open?id=1GrepBq4JiV4LZ9lvslF8ygDzxrgg8WzD)\n",
    "(Where we keep all of the intermediary outputs of extracted features, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive data exploration and visualization was conducted to understand the data better for feature engineering and strategy for choosing our algorithms toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Data Exploration Notebook](data_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction/engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering was composed of 4 main steps:\n",
    "* #### Data Imputation (see below)\n",
    "* #### [Image Feature Engineering Notebook](./feature_engineering/image_feature_engineering.ipynb)\n",
    "* #### [Text Feature Engineering Notebook](./feature_engineering/nlp_feature_engineering.ipynb)\n",
    "* #### [Categorical Feature Engineering](./feature_engineering/feature_enrichment.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leveraged deep learning models to learn missing data of important features. We learned the important features only after this step, and then re-iterated the whole learning with the imputated data. We recommend reading those notebooks only after getting familiar with our Neural Network models (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Imputating image_top_1 as continuous with NN notebook](./feature_engineering/NN-Stephan-LearnImageTop1-regression.ipynb)\n",
    "#### [Imputating image_top_1 as a class with NN notebook](./feature_engineering/NN-Stephan-LearnImageTop1.ipynb)\n",
    "#### [Imputating price with NN notebook](./feature_engineering/NN-Stephan-LearnPrice.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***./utils/feature_enrichment.py*** module contains the loading functions for some of the resulted feature engineering work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All outputs of the various feature engineering tasks are outputted as the following files and loaded as features before running the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithms, Models and Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm family that comes to mind after exploring the data and extracting features is the **Decision Tree** family. Many algorithms are know in the field, while ensembling methods rule with superior performance and speed, between three popular decisions: *XGBoost*, *CatBoost* and *LGBM*, we finally chose **LGBM** after also testing *CatBoost* with lower performance (XGboost wasn't considered since it's inferior when dealing with heavy categorical data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run some basic algorithms to produce naive results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Naive Learning Notebook](./algorithms/naive_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost is a competitor of LGBM, but with inferior results as seen in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [CatBoost Notebook](./algorithms/catboost.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained LGBM for a few iterations, and improved the features and hyperparameters in every iteration until we got our best score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [LGBM - Final iteration Notebook](./algorithms/lgbm_final-0.2281.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM is a great candidate here since there are many categorical features, but Deep Learning is a very strong family of classifiers and can many times produce better results.\n",
    "As mentioned in the documentation, We had trained 10 neural networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF text vectorization:\n",
    "\n",
    "#### [NN-TFIDF-SEPARATED-UNIGRAMS](./NN/NN-Stephan-NN-TFIDF-SEPARATED-UNIGRAMS-nodropout.ipynb)\n",
    "\n",
    "#### [NN-TFIDF-SEPARATED-BIGRAMS](./NN/NN-Stephan-NN-TFIDF-SEPARATED-BIGRAMS-nodropout.ipynb)\n",
    "\n",
    "#### [NN-TFIDF-MERGED-UNI](./NN/NN-Stephan-NN-TFIDF-MERGED-UNI-nodropout.ipynb)\n",
    "\n",
    "#### [NN-TFIDF-MERGED-BIGRAMS](./NN/NN-Stephan-NN-TFIDF-MERGED-BIGRAMS-nodropout.ipynb)\n",
    "\n",
    "CountVec text vectorization:\n",
    "\n",
    "#### [NN-CountVec-SEPARATED-UNIGRAMS](./NN/NN-Stephan-NN-CountVec-SEPARATED-UNIGRAMS-nodropout.ipynb)\n",
    "\n",
    "#### [NN-CountVec-SEPARATED-BIGRAMS](./NN/NN-Stephan-NN-CountVec-SEPARATED-BIGRAMS-nodropout.ipynb)\n",
    "\n",
    "#### [NN-CountVec-MERGED-UNI-nodropout](./NN/NN-Stephan-NN-CountVec-MERGED-UNI-nodropout.ipynb)\n",
    "\n",
    "#### [NN-CountVec-MERGED-BIGRAM](./NN/NN-Stephan-NN-CountVec-MERGED-BIGRAMS-nodropout.ipynb)\n",
    "\n",
    "FastText word embeddings + LSTM text processing (Trained on GCP, need ~50GB to run):\n",
    "\n",
    "#### [NN-LSTM-MERGED-TEXT-COMBINED-NODROPOUT](./NN/NN-Stephan-LSTM-MERGED-TEXT-COMBINED-NODROPOUT.ipynb)\n",
    "\n",
    "#### [NN-JUST-LSTM-MERGED-NODROPOUT](./NN/NN-Stephan-JUST-LSTM-MERGED-NODROPOUT.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling\n",
    "For the general ensembling we've use two techniques, a meta-network (for non linear ensembling) and a lasso regression model (for linear ensembling). Those get as input the predictions of all other models and give the final prediction as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [The general ensemble](./NN/NN-Stephan-Ensemble.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
