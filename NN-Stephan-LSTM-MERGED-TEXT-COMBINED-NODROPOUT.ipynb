{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Combined - LSTM for (merged) text + Dense for others\n",
    "\n",
    "Our NN models vary in the way we preprocess the text features (most important feature) towards feeding it into our NN.\n",
    "This also effects the architechture of the NN.\n",
    "\n",
    "In this notebook we:\n",
    "1. Merge all text features (Title, Description, params) into one feature.\n",
    "2. We learn the representation of the text feature by an LSTM (its output vector is the text representation).\n",
    "3. We combine the LSTMed text with all other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilaif/.virtualenvs/ds/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "HELPER_DATA_PATH = './helper_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "Adding basic features...\n",
      "Done adding basic features.\n",
      "Adding image features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilaif/git/ds-workshop/feature_enrichment.py:74: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/Users/ilaif/git/ds-workshop/feature_enrichment.py:75: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading image features.\n",
      "Loading text features...\n",
      "loading tfidf features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bd014056578d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_enrichment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHELPER_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHELPER_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_text_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHELPER_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_aggregated_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHELPER_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric_features_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHELPER_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ds-workshop/feature_enrichment.py\u001b[0m in \u001b[0;36mload_text_features\u001b[0;34m(train, test, helper_data_path, tf_idf)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmore_text_count_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmore_text_count_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ds-workshop/feature_enrichment.py\u001b[0m in \u001b[0;36mmore_text_count_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unique_words_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_desc_punct'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words_vs_unique_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unique_words_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ds/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/git/ds-workshop/feature_enrichment.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unique_words_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_desc_punct'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words_vs_unique_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unique_words_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ds-workshop/feature_enrichment.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(l1, l2)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmore_text_count_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unique_words_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ds-workshop/feature_enrichment.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmore_text_count_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_unique_words_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print('loading data...')\n",
    "# train, test = load_data(DATA_PATH)\n",
    "# train, test = basic_enrichment(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "# train, test = load_image_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "# train, test = load_text_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "# train, test = add_aggregated_features(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "# train, test = numeric_features_cleaning(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "# train, test = complete_image_top_1(train, test, helper_data_path=HELPER_DATA_PATH)\n",
    "# train, test = complete_price(train, test, helper_data_path=HELPER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize features towards input to an NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_features = ['title', 'description', 'param123']\n",
    "text_feature = 'title_description_params'\n",
    "cat_features = ['user_type', \\\n",
    "                'region', 'city', \\\n",
    "                'parent_category_name', 'category_name', 'param_1', 'param_2', 'param_3', \\\n",
    "                'image_top_1_class', 'image_top_1_rounded_regression', \\\n",
    "                'month', 'day', 'weekday', \\\n",
    "                'has_price', 'has_description', 'has_params', 'has_image']\n",
    "cont_ord_features = ['image_top_1_regression', \\\n",
    "                     'log_price_regression', \\\n",
    "                     'avg_days_up_user', 'avg_times_up_user', 'n_user_items', 'user_ads_count', \\\n",
    "                     'log_item_seq_number', \\\n",
    "                     'img_size', 'img_luminance', 'img_colorfulness', 'img_confidence', 'log_img_sharpness', 'log_img_keypoints', \\\n",
    "                     'title_word_count', 'description_word_count', 'merged_params_word_count', \\\n",
    "                     'description_non_regular_chars_ratio', 'title_capital_letters_ratio','description_capital_letters_ratio', \\\n",
    "                     'title_non_regular_chars_ratio', 'title_adj_to_len_ratio', 'title_noun_to_len_ratio',\\\n",
    "                     'title_sentiment']\n",
    "\n",
    "train_y_prob = train['deal_probability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text towards input to an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "stopwords = set()\n",
    "with codecs.open(os.path.join(HELPER_DATA_PATH, 'stopwords_ru.txt'), encoding='cp1251') as ins:\n",
    "    for w in ins:\n",
    "        word = w.strip(\"\\r\\n\")\n",
    "        word = word.strip(\"\\n\")\n",
    "        stopwords.add(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and Vectorize (keras encoded one-hot representation (each onehot vec represented as an int number)) text feature.\n",
    "\n",
    "See: https://keras.io/preprocessing/text/#one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot, Tokenizer, text_to_word_sequence\n",
    "\n",
    "# Those consts are important for the NN itself\n",
    "max_words_in_desc_title_param = 150 # See text analysis notebook. 95% are shorter than a 110.\n",
    "word_embed_dim = 300\n",
    "\n",
    "train_x_text = train[[text_feature]]\n",
    "test_x_text = test[[text_feature]]\n",
    "train_x_text[text_feature] = train_x_text[text_feature].str.lower()\n",
    "test_x_text[text_feature] = test_x_text[text_feature].str.lower()\n",
    "tokenizer = Tokenizer(num_words = max_words_in_desc_title_param)\n",
    "all_text = np.hstack([train_x_text[text_feature], test_x_text[text_feature]])\n",
    "\n",
    "print('Tokenizing text...')\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "print('Done tokenizing.')\n",
    "\n",
    "del all_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# By default text_to_word_sequence automatically does 4 things:\n",
    "#   Splits words by space (split=” “), \n",
    "#   Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n",
    "#   Converts text to lowercase (lower=True).\n",
    "# We add stopwords filtering to the process.\n",
    "\n",
    "def my_text_to_word_sequence(text):\n",
    "    result = []\n",
    "    for word in text_to_word_sequence(text):\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Applying tokenizer on text...')\n",
    "\n",
    "train_x_text[text_feature] = train_x_text.apply(lambda r: [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "test_x_text[text_feature] = test_x_text.apply(lambda r: [tokenizer.word_index[word] for word in my_text_to_word_sequence(r[text_feature])], axis=1)\n",
    "    \n",
    "train_x_text = pad_sequences(train_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "test_x_text = pad_sequences(test_x_text[text_feature], maxlen=max_words_in_desc_title_param)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and load FastText (Facebook's) Russian wikipedia word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fasttext_embedding_matrix(data_path, tokenizer, embedding_dim):\n",
    "    print('loading embeddings...')\n",
    "    \n",
    "    EMBEDDING_FILE_PATH = os.path.join(data_path, 'cc.ru.300.vec')\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_PATH))\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 2\n",
    "    embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "    \n",
    "    print('creating embedding matrix...')\n",
    "    embedding_exists = 0\n",
    "    no_embeddings = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "            embedding_exists += 1\n",
    "        else:\n",
    "            no_embeddings += 1\n",
    "    \n",
    "    print (\"There are total of {} words in our corpus.\".format(embedding_exists+no_embeddings))\n",
    "    print (\"There are {} embeddings in FastText.\".format(len(embeddings_index)))\n",
    "    print (\"We have embeddings for {} words ({}% existing embeddings).\".format(embedding_exists, \\\n",
    "                                                                               (100*embedding_exists/(embedding_exists+no_embeddings))))\n",
    "    print (\"Embedding is missing for {} words.\".format(no_embeddings))\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    \n",
    "    print('done loading embeddings...')\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix, vocab_size = get_fasttext_embedding_matrix(data_path=DATA_PATH, \\\n",
    "                                                             tokenizer=tokenizer, embedding_dim = word_embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features.\n",
    "\n",
    "Vectorize all loaded categorical features.\n",
    "\n",
    "\n",
    "See: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x_cat = train[cat_features]\n",
    "test_x_cat = test[cat_features]\n",
    "for col in cat_features:\n",
    "    train_x_cat[col] = train_x_cat[col].astype('category')\n",
    "    test_x_cat[col] = test_x_cat[col].astype('category')\n",
    "\n",
    "# Encode to integers.\n",
    "# For vectorization (encoding) we concat both train and test into one\n",
    "all_cat = pd.concat([train_x_cat, test_x_cat], axis = 0)\n",
    "for col in cat_features:\n",
    "    enc = preprocessing.LabelEncoder().fit(all_cat[col])\n",
    "    train_x_cat[col] = enc.transform(train_x_cat[col])\n",
    "    test_x_cat[col] = enc.transform(test_x_cat[col])\n",
    "\n",
    "# One-hot encode:\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(pd.concat([train_x_cat, test_x_cat], axis = 0))\n",
    "train_x_cat = enc.transform(train_x_cat)\n",
    "test_x_cat = enc.transform(test_x_cat)\n",
    "\n",
    "del all_cat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical(Continious/Ordinal) features\n",
    "\n",
    "Normalize all loaded numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_numerical = train[cont_ord_features]\n",
    "test_x_numerical = test[cont_ord_features]\n",
    "train_x_numerical.fillna(0, inplace = True)\n",
    "test_x_numerical.fillna(0, inplace = True)\n",
    "for col in cont_ord_features:\n",
    "    train_x_numerical[col] = train_x_numerical[col].astype('float64')\n",
    "    test_x_numerical[col] = test_x_numerical[col].astype('float64')\n",
    "\n",
    "# Normalize features:\n",
    "train_x_numerical = normalize(train_x_numerical, axis=0)\n",
    "test_x_numerical = normalize(test_x_numerical, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning - Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should have a pad sequence out of the text features\n",
    "All categorials should be onehot encoded\n",
    "All continious/ordinal should be vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(): Try hidden_dim1 = 512, hidden_dim2 = 128 after loading FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_out_dim = 256\n",
    "cat_hidden_dim = 128\n",
    "merged_hidden_dim = 256\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that gets a tokenizer, and returns fasttext_embedding_matrix that will be loaded into the embedding layer of our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "l_text_input = Input(shape=(max_words_in_desc_title_param,), name=\"text_lstm_input\")\n",
    "# Load FastText's weights:\n",
    "l_text_embedding = Embedding(input_dim=vocab_size, output_dim=word_embed_dim, weights = [embedding_matrix], \\\n",
    "                             input_length=max_words_in_desc_title_param, name='text_embeddings')(l_text_input)\n",
    "lstm_out = LSTM(lstm_out_dim, name='LSTM')(l_text_embedding)\n",
    "\n",
    "# Categoricals\n",
    "l_cat_input = Input(shape=(train_x_cat.shape[1],), sparse=True, name=\"cat_input\")\n",
    "l_hidden_cat = Dense(cat_hidden_dim, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(1e-6), name='l_hidden_cat')(l_cat_input)\n",
    "\n",
    "# Numerical\n",
    "l_numerical_input = Input(shape=(train_x_numerical.shape[1],), name=\"numerical_input\")\n",
    "\n",
    "# Aggregate all inputs into one hidden layer.\n",
    "l_aggregative = concatenate([lstm_out, l_hidden_cat, l_numerical_input])\n",
    "\n",
    "l_merged_hidden = Dense(merged_hidden_dim, activation='relu',\n",
    "                            kernel_regularizer=regularizers.l2(1e-6), name='l_merged_hidden')(l_aggregative)\n",
    "\n",
    "output = Dense(out_dim, activation='sigmoid', name='output')(l_merged_hidden) # This is the logistic regression output\n",
    "\n",
    "rmsprop_opt = RMSprop(lr=0.0001) # Best for training RNNs.\n",
    "def rmse_err(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "model = Model(inputs=[l_text_input, l_cat_input, l_numerical_input], outputs=output)\n",
    "model.compile(optimizer=rmsprop_opt ,loss=[rmse_err]) # Higher weight for main output.\n",
    "print(model.summary())\n",
    "\n",
    "load = False\n",
    "fname = 'NN-LSTM-COMBINED-MERGED-NODROPOUT-model-weights.h5'\n",
    "if load:\n",
    "    model.load_weights(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce_lr_cd = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "save_weights_cd = ModelCheckpoint(fname, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "nbatch_size = 512\n",
    "\n",
    "model.fit({'text_lstm_input': train_x_text, 'cat_input': train_x_cat, 'numerical_input': train_x_numerical}, train_y_prob, \n",
    "          validation_split = 0.1, \n",
    "          epochs=3, \n",
    "          batch_size=nbatch_size,\n",
    "          callbacks=[reduce_lr_cd, save_weights_cd])\n",
    "\n",
    "# Train on all data\n",
    "model.fit({'text_lstm_input': train_x_text, 'cat_input': train_x_cat, 'numerical_input': train_x_numerical}, train_y_prob, \n",
    "          epochs=2, \n",
    "          batch_size=nbatch_size,\n",
    "          callbacks=[save_weights_cd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions to futher ensambels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_train = model.predict({'text_lstm_input': train_x_text, 'cat_input': train_x_cat, 'numerical_input': train_x_numerical})\n",
    "y_pred_test = model.predict({'text_lstm_input': test_x_text, 'cat_input': test_x_cat, 'numerical_input': test_x_numerical})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_train_df = pd.DataFrame(train['item_id'])\n",
    "res_train_df['deal_probability'] = y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_test_df = pd.DataFrame(test['item_id'])\n",
    "res_test_df['deal_probability'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df = pd.concat([res_train_df, res_test_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('./predictions/NN-LSTM-COMBINED-MERGED.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
